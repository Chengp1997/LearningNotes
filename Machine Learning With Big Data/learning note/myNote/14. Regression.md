# Regression

要预测的东西不再是离散的，而是 **continuous**，是可以具体到数字的！！

## Linear Regression

> It helps to think of linear regression as fitting a line to a set of data, but this is a special case for a 2 dimensional dataset (1 target and one 1 feature). In the 3 dimensions (1 target and 2 features) we are actually fitting a plane and in higher dimensions we are fitting a hyper plane.

简单来说，就是画函数

**难点**：寻找系数（coefficient）

linear least squares 最小：也就是要寻找距离标准值，方差最小的系数。

<img src="../note picture/Regression1.png" alt="image-20211018184144399" style="zoom:25%;" />

P.S.：可以选择使用垂直线来计算差异

### 使用

一维：只有一个feature

```python
from sklearn import linear_model

#Splitting training and testing data (features)
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]

#Splitting training and testing data (target)
diabetes_y_train = diabetes.target[:-20]
diabetes_y_test = diabetes.target[-20:]

# Linear regression
# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit(diabetes_X_train, diabetes_y_train)

# The coefficients
print('Coefficients:', regr.coef_)
# The mean square error
print("Mean Squared Error: %.2f"
      % np.mean((regr.predict(diabetes_X_test) - diabetes_y_test) ** 2))
# Explained variance score: 1 is perfect prediction
print('R^2 score: %.2f' % regr.score(diabetes_X_test, diabetes_y_test))

# Plot outputs
plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')
plt.plot(diabetes_X_test, regr.predict(diabetes_X_test), color='blue',
         linewidth=3)

plt.show()
# 显示的是正常我们所看到的二维图
```

多维：有多个feature

**难点：**如何作图There's a couple things you can do: 1) you can perform dimensionality reduction but then your line is no longer a line, or you can slice the 11 dimensions into sets of 2 dimensions by setting all the other dimensions to a value, common choices are the origin (0) or the mean.



## Logistic Regression

是 Naive Bayes 和Linear Regression 的结合体

适合用来预测binary regression

<img src="../note picture/regression2.png" alt="image-20211018184909146" style="zoom:25%;" />

<img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20211018203437051.png" alt="image-20211018203437051" style="zoom:50%;" />

- p 是 时间发生的概率
- LHS：优势比的log
- RHS：线性回归

```python
from sklearn import linear_model, datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
y = iris.target

logreg = linear_model.LogisticRegression(solver="lbfgs", multi_class="auto")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

logreg.fit(X_train, y_train)
probs = logreg.predict_proba(X_test)
print(probs)

predicted = logreg.predict(X_test)

print(metrics.confusion_matrix(y_test, predicted))
print(metrics.classification_report(y_test, predicted))

metrics.plot_confusion_matrix(logreg, X_test, y_test).ax_.grid(False)  # the seaborn library adds grids to everything
# so we turn that off 
```



## KNN Regression

Combine predictions for nearby points

- Averaging
- Interpolation
- Local linear regression
- Local weighted regression


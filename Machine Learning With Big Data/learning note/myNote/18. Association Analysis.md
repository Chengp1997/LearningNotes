# Association Analysis

### 定义

主要是 ： unsupervised

也叫：market basket analysis; mining transaction datasets; itemset mining; association rule mining

简而言之，就是寻找关联，看看什么事物之间关联比较强。

### 方法

- Frequent itemset: 找到出现频率最高的item「X,Y」（SUPPORT）
- Association Rules: 寻找是否X->Y或者Y->X有关联 （CONFIDENCE）

#### Rule evaluation metrics

Support(s):**Fraction** of transactions that contain both X and Y

Confidence(c):Measures **how often** items in Y appear in transactions that contain X

同样的itemset可以有多种组合，因此也有多个support和confidence的值。

<img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa1.png" alt="image-20211110162458852" style="zoom: 25%;" />

因此有：

#### Two step approach

<img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa2.png" alt="image-20211110170714695" style="zoom:25%;" />

#### Frequent Itemset

一般有k个项的itemset一共有2^k-1种选项集数，如下展示了{a,b,c,d,e}的集合

<img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa3.png" alt="image-20211110171000411" style="zoom: 25%;" />

下一步就是计算support支持度。

最笨的方法是所有都计算一遍，找最小。

可以用如下方法减少计算复杂度

- Apriori principle：减少候选项集的数目，可以不用计算支持度而删除某些项

  > 先验原理：如果一个项集是频繁的，则它的所有子集都是频繁的；如果一个项集是非频繁的，则它的所有超集也一定是非频繁的。

  简单来说，如果support度不高，后面的都可

  如下，

  当只有一个item的时候，coke和egg由于support太低，因此删除

  当有两个item的时候，可以不包含coke和egg了

  最后由2项集生成3项集

  <img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa4.png" alt="image-20211110172137156" style="zoom:25%;" />

  <img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20211110171625212.png" alt="image-20211110171625212" style="zoom:25%;" />

##### 算法：

<img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa5.png" alt="image-20211110180621818" style="zoom:25%;" />

#### Rule Generation

**规则：**

<img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa6.png" alt="image-20211110181454576" style="zoom:25%;" />

对于含有同样项的有这样的规则：注意看关系右边。剪枝的条件就是如果含有右边项的都可以去除

<img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa7.png" alt="image-20211110182422588" style="zoom:50%;" />

<img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa8.png" alt="image-20211110182850521" style="zoom: 25%;" />



## Recommender System

- Content-based filtering
  - A喜欢B，所以A喜欢C
- Collaborative filtering
  - A喜欢D喜欢的B，D喜欢C，所以A喜欢C

### Collaborative filtering

主要有两种方式：user-based和item-based，两种都是

#### user based: 

<img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa9.png" alt="image-20211110191709685" style="zoom:25%;" />

通过如上方式计算用户之间的相似度。

<img src="/Users/chengeping/Documents/LearningMaterial/VTNote/CS5644 machine learning with big data/note picture/aa10.png" alt="image-20211110191827280" style="zoom:25%;" />

通过用户之间的相似性和他们的评分来进行最终预测。

**缺点：**如果用户太多，item太少，意义不大。

#### item based:

和上述方式类似，只是计算的是item之间的相似性。

计算除了要预测的用户的其他用户对同一个item的相似性。寻找和要寻找的item相似度最高的item。


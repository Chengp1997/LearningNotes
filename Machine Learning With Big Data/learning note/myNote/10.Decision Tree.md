#  Decision Tree

https://zhuanlan.zhihu.com/p/30059442 

<img src="../note picture/decision treeExample.png" alt="image-20211002154737535" style="zoom:80%;" />

## 定义

**分类算法！**

- 通过从root到leaf的顺序进行分类。
- 每个节点都是一个attribute
- 每个分支都代表了一个值

你只需要把数据喂给算法，它会自己分类！

一棵好的决策树，不仅仅将给定表格体现出来，还会descard一些不合适的实体类，并且适应性强！！！（对新的情况也同样适用）

## 术语

### **变量**

**难点！**

如何定义节点内容**（变量内容）**-- 才能保证减少不确定性，又不会使得树太浅/太深

> **A good question is one that reduces the entropy**

一个好的变量可以清晰地把数据分为正负两类，不会有模棱两可的地方。

### **Entropy** (信息熵)

> Characterizes the (im)purity of arbitrary collection of examples

一般来说，**熵越小，分类越好**

对于只有两种值的变量，entropy有：

​																	<img src="../note picture/entropyFunction.png" alt="image-20211002172246510" style="zoom: 33%;" />

对于多值的类型，entropy 有：

​																	<img src="../note picture/image-20211002165146501.png" alt="image-20211002165146501" style="zoom:25%;" />



### **Information Gain 信息增益**

https://www.zhihu.com/question/22104055 

**信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度。**

***信息增益越大，说明这个attribute越好***

<img src="../note picture/informationGain.png" alt="image-20211002170344522" style="zoom:50%;" />

<img src="../note picture/ExampleEntropy.png" alt="image-20211002172814810" style="zoom: 33%;" />

<img src="../note picture/InformationGainExample.png" alt="image-20211002173101347" style="zoom:33%;" />

因为Hot temp的信息增益最大，因此这个变量最好，可以选择它最为splitting attribute，也就是父节点。

**总结**

熵：表示随机变量的不确定性。

条件熵：在一个条件下，随机变量的不确定性。

信息增益：熵 - 条件熵。表示在一个条件下，信息不确定性减少的程度。

## 应用

什么是一棵好的决策树？

如果

- 数据太杂乱--只注意变化的数据而忽略底层规律
- 树太深（变量过多）-- 可能会不适应变化
- 树太浅（变量不多）-- 可能不会很好地找到底层规律

https://zhuanlan.zhihu.com/p/30059442 

### ID3

这是一种贪心算法。

由增熵（Entropy）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。

### C4.5

通过对ID3的学习，可以知道ID3存在一个问题，那就是越细小的分割分类错误率越小，所以ID3会越分越细. 可能会导致过度学习。

所以为了避免分割太细，c4.5对ID3进行了改进，C4.5中，优化项要除以分割太细的代价，这个比值叫做信息增益率，显然分割太细分母增加，信息增益率会降低。除此之外，其他的原理和ID3相同。

### CART





## 使用

import package

```python
import sklearn
import numpy as np
import pandas as pd

import graphviz
import sklearn.tree
from io import StringIO
```

load data

```python
col_names = "rain,hot temp.,mild temp.,High Humidity,Strong winds,playTennis"
my_tennis_data = np.array([[0, 1, 0, 1, 0, 0],
       [0, 1, 0, 1, 1, 0],
       [1, 0, 1, 1, 0, 1],
       [1, 0, 0, 0, 0, 1],
       [1, 0, 0, 0, 1, 0],
       [0, 0, 1, 1, 0, 1],
       [0, 0, 0, 0, 0, 1],
       [1, 0, 1, 0, 0, 1],
       [0, 0, 1, 0, 1, 1],
       [1, 0, 1, 1, 1, 0]])
tennis_df = pd.DataFrame(data=my_tennis_data, columns=col_names.split(","))

### select all rows except last column
X = tennis_df.drop(["playTennis"], axis=1)
### select last column
y = tennis_df["playTennis"]
```

use model

```python
### Initiate classifier
infoGain_clf = sklearn.tree.DecisionTreeClassifier(criterion='entropy')

### Fit classifier
infoGain_clf_tree = infoGain_clf.fit(X, y)
```

visualize tree

```python
ss = StringIO()

feature_names = X.columns
label_name = ["Dont play Tennis", "Play Tennis"]

sklearn.tree.export_graphviz(infoGain_clf_tree, feature_names=feature_names,
                             class_names=label_name,
                             label="all", out_file=ss)

graphviz.Source(ss.getvalue())
```

predict

```python
print("predicted:", infoGain_clf.predict(X[-2:]))
print("truth", y[-2:].values)
```


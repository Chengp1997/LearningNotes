# Scikit-Learn

## Basics

### import

分类评估模型

https://zhuanlan.zhihu.com/p/37654241 



## Cross validation

为何要cross validation

https://zhuanlan.zhihu.com/p/24825503

### 使用

Import

```python
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd 

import sklearn
from sklearn import tree
from sklearn import datasets
from sklearn import model_selection
from sklearn import metrics%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd 

import sklearn
from sklearn import tree
from sklearn import datasets
from sklearn import model_selection
from sklearn import metrics
```

load data

```python
iris = datasets.load_iris()
tmp = {name: iris.data[:, i] for i, name in enumerate(iris.feature_names)}  # dictionary comprehension
# print(iris.data)
print(iris.feature_names)
tmp["target"] = [iris.target_names[i] for i in iris.target]
iris_df = pd.DataFrame(tmp)
iris_df.columns = ["sepal_length", "sepal_width", "petal_length", "petal_width", "target"]
iris_df.head()
```

use model

```python
X = iris_df.drop(["target"], axis=1) # 除了最后一列的所有数据
y = iris_df["target"] #最后一列的所有
clf = tree.DecisionTreeClassifier()  #classifier!
# print(clf)
```

**cross validation**

```python
#如果不使用交叉验证，只是简单的分成两部分，可能会有的问题：见上面链接
(X_train, X_test, y_train, y_test) = model_selection.train_test_split(X, y, test_size=1/3) #split! 
# 机器学习一般会把数据分成训练数据和测试数据。 右边括号中，相当于---x为训练数据，y为目标数据，所要划分的样本结果。test_size为 样本占比
# 这里的例子只是简单地进行分区，下面的例子是使用交叉验证的方式
print(X_train.shape, X_test.shape) 

#以下使用交叉验证的方式
kf = model_selection.KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(iris_df):
    print("TRAIN:", train_index[:5], "TEST:", test_index[:5])
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print("precision={}".format(metrics.precision_score(y_test, y_pred, average="weighted")))
    print("recall=   {}".format(metrics.recall_score(y_test, y_pred, average="weighted")))
    print("f1=       {}".format(metrics.f1_score(y_test, y_pred, average="weighted")))
    
    print()
    
```

Fit model

```python
#train 应用到决策树中
clf.fit(X_train, y_train)
#predict  预测结果
y_pred = clf.predict(X_test)
y_pred
```

evaluate

```python
# 当不使用交叉验证时，简单地进行评分。
# score  https://zhuanlan.zhihu.com/p/37654241 
# precision_score: 查准率，看的是我们所关注的类别正确分类的比率
# recall_score: 召回率，即真实正例中最后预测为正例所占的比例
# f1 score：为准确率和召回率的组合，常作为模型选择的指标
print("precision={}".format(metrics.precision_score(y_test, y_pred, average="weighted")))
print("recall=   {}".format(metrics.recall_score(y_test, y_pred, average="weighted")))
print("f1=       {}".format(metrics.f1_score(y_test, y_pred, average="weighted")))

output：
precision=0.9810526315789474
recall=   0.98
f1=       0.9798998998998999

#当使用交叉验证时
scores = model_selection.cross_validate(clf, X, y, cv=5, 
                                        scoring=["precision_weighted", "recall_weighted", "f1_weighted"])
scores：
{'fit_time': array([0.0035162 , 0.00289297, 0.00248313, 0.00224376, 0.00220323]),
 'score_time': array([0.00488663, 0.00595093, 0.00410891, 0.00372291, 0.00403881]),
 'test_precision_weighted': array([0.96969697, 0.96969697, 0.9023569 , 1.        , 1.        ]),
 'test_recall_weighted': array([0.96666667, 0.96666667, 0.9       , 1.        , 1.        ]),
 'test_f1_weighted': array([0.96658312, 0.96658312, 0.89974937, 1.        , 1.        ])}

np.mean(scores["test_precision_weighted"]) #最后计算总的

output：
0.9683501683501683
```



## evaluation

https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter


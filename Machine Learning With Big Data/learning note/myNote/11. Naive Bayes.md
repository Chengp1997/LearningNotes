# Naive  Bayes Classifier

https://www.cnblogs.com/pinard/p/6069267.html 

## Probability

### Empirical relative frequency (Frequentist) 

时间的经验概率

也就是我们正常理解的经验概率。经典例子--扔硬币，两种可能的概率都是0.5。通过不断实验获取经验来判断

### Degree of belief (Bayesian)

就是条件概率。在判断概率的时候，还需要考虑前置概率。

条件独立公式：

**P(A,B)=P(A)*P(B)**

条件概率公式

**P(A|B) X P(B) = P(B|A) X P(A)**

**P(A|B) = P(B|A) X P(A) X (1/P(B))**

其他特性

**P(A,B) = P(B,A)**

**P(A|B) = P(A,B)/P(B) **

**P(B|A) = P(A,B)/P(A)**

于是有贝叶斯公式

<img src="../note picture/BayesFunction.png" alt="image-20211003115930774" style="zoom: 25%;" />

## How it works

<img src="../note picture/ExampleBayes.png" alt="image-20211003113209000" style="zoom:50%;" />

目标：给定一个测试集，找，应该是Yes还是No，换句话说，看看Yes和No谁的概率更大就是什么。

​    P(class1|features) >< P(class2|features)

= P(features|class1) X P(class1) >< P(features|class2) X P(class2)

= P(f1|class1) X P(f2|class1) X ... X P(class1) >< P(f1|class2) X P(f2|class2) X ... X P(class2)

对于例子有：

{Sunny, No, No, High, Weak}

LHS = P(features|class1) = P(Sunny|Yes) X P(No|Yes) X P(No|Yes) X P(High|Yes) X P(Weak|Yes)

​		=3/6x...x...x2/6x5/6

RHS = P(features|class2) = P(Sunny|No) X P(No|No) X P(No|No) X P(High|No) X P(Weak|No)

​		= 2/4x...x...x3⁄4x1⁄4

因此可以得出，Yes的几率更大。

## Example：Text Classification

数据描述：

给定的数据为：email，paragraph，post……

目标class：找到spam，非spam

**data preprocessing**

- Organize a corpus of training documents 整理训练集
- Junk formatting and unformatting 
- Standardize case 标准化？
- Tokenization (“Dear Mr. O’Neill” => {Dear, ...}) • Stopword removal 标记
- Normalization 规范化
  - Accents, diacritics
  - Stemming (e.g., Porter’s stemmer) vs Lemmatization

**find features**

<img src="../note picture/findFeatures.png" alt="image-20211003124906309" style="zoom: 33%;" />

如果一个词未出现，则有

- 使用拉普拉斯定理

<img src="../note picture/laplas.png" alt="image-20211003125711242" style="zoom:33%;" />

但是，以上方法给unseen words过多权重，于是有

- GTestimator：

Idea:iftwowordsappearequalnumberoftimes, they have the same probability 将相同的词划分同类

- Bernoulli model



选择features 策略

**Strategies**

- Mutual information between terms and classes 
- Chi-squared test between terms and classes
- Frequency of terms in a given class

Rank features for each class and union them into a set

对于有连续值的：使用Gaussian NBC



## 使用

example：play tennis

需要的包

```python
import sklearn
import sklearn.tree
import numpy as np
from sklearn.naive_bayes import BernoulliNB
```

load data

```python
col_names = "rain,hot temp.,mild temp.,High Humidity,Strong winds,playTennis"
my_tennis_data = np.array([[0, 1, 0, 1, 0, 0],
       [0, 1, 0, 1, 1, 0],
       [1, 0, 1, 1, 0, 1],
       [1, 0, 0, 0, 0, 1],
       [1, 0, 0, 0, 1, 0],
       [0, 0, 1, 1, 0, 1],
       [0, 0, 0, 0, 0, 1],
       [1, 0, 1, 0, 0, 1],
       [0, 0, 1, 0, 1, 1],
       [1, 0, 1, 1, 1, 0]])
       
### select all rows except last column
X = my_tennis_data[:, :-1] 
### select last column
y = my_tennis_data[:, -1]
label_name = ["Dont play Tennis", "Play Tennis"]
```

use model

```python
bayes_clf = BernoulliNB()
bayes_clf.fit(X, y)
```

predict

```python
print("predicted:", bayes_clf.predict(X[-2:,:]))
print("truth", y[-2:])
```


# NeuralNets

## 神经元原理

- 一个神经元能收到其他神经元传输过来的东西
- input一般是多个神经元的合
- 当input超过一个阈值，神经元就会发送一个电子信号给下一个神经元

一个神经元

<img src="../note picture/neural.png" alt="image-20211102161926317" style="zoom: 50%;" />

## 演变

Idealized Neuron! 去除无关信息，尽量使其变成一个“完美的”神经元，然后一点点推进。

### Perceptron 

<img src="../note picture/perceptron.png" alt="image-20211102174813981" style="zoom:50%;" />

- 过程：
  - 猜测weight
  - fill in input
  - 查看是否是对应output
  - 然后调整，iterate
- 是一个很好的线性分类器，是一个idealized neuron，
  - 训练数据是线性可分的
  - 足够小的learning rate
- 缺点
  - 但是只可以是线性分类器
  - 如果让它来跑XOR的data，weight会不停变动
  - 可能解会有很多种，只会是局部最优

<img src="../note picture/trainingRule.png" alt="image-20211103094401886" style="zoom:50%;" />

limitation

<img src="../note picture/limitation.png" alt="image-20211103102610689" style="zoom: 33%;" />

#### 例子：

iterative approach 首先猜测价格，然后慢慢调整直至逼近，高了降低，低了提高

<img src="../note picture/exampleNeural.png" alt="image-20211102191514801" style="zoom:50%;" />

guess为最开始猜测的神经元图，猜测的数据得出最后price为500

和实际price相差350

于是delta-rule 来猜测w有。dieta W= 1/35*xi（850-500）

于是每次更改的数据为 20，50，30

然后继续慢慢调整。

### Error surface

如图，水平方向为weight，垂直方向是error

垂直方向的差异是抛物线，水平方向差异为椭圆

他们互相垂直

简单的batch learning， 坡度可能会很抖！

<img src="../note picture/erroSurface.png" alt="image-20211102231207500" style="zoom:50%;" />

### Gradient Descent

梯度下降

<img src="../note picture/GradientDescent.png" alt="image-20211102235600083" style="zoom:50%;" />



### Neuron Models

通过分析上述的例子，我们可以将整个过程高度概念化为一下形式。

关于**激活函数**

https://zhuanlan.zhihu.com/p/32824193 

最后神经网络分类成什么样，应该说取决于激活函数是什么

> 因为我们最终要解决一个分类问题（classification）

那么可以理解成，最后分类的结果，取决于激活函数如何走向



常见激活函数

<img src="../note picture/function.png" alt="image-20211103013555329" style="zoom:50%;" />

#### 一个例子

sigmoid unit： 激活函数为sigmoid

<img src="../note picture/sigmoid.png" alt="image-20211103014239894" style="zoom:50%;" />

对于线性神经元，有：

<img src="../note picture/sigmoidFunction.png" alt="image-20211103020758529" style="zoom:50%;" />

### 神经网络

<img src="../note picture/NeuralNetwork.png" alt="image-20211103102728097" style="zoom:50%;" />

### Back Propagation 反向传播算法

https://www.cnblogs.com/charlotte77/p/5629865.html

<img src="../note picture/Procedure.png" alt="image-20211103131208299" style="zoom:50%;" />

所谓deep learning，是因为隐含层多，神经网络很深，所以是“deep” learning

<img src="../note picture/ShallowAndDeep.png" alt="image-20211103135407228" style="zoom:50%;" />

其实只需要一两层就够了，但是！更深层次的神经网络可以更加模块化地理解数据。例如图像处理，某一层可能学习的就是亮度。

<img src="../note picture/Step.png" alt="image-20211103134842305" style="zoom:50%;" />
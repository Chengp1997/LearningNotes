## Intro

### Offline Planning

- MDP
- Agent ==**have full knowledge** of transition function and reward function== 
- **Precompute** optimal actions encoded by the MDP without ever actually taking any actions
- ç®€è€Œè¨€ä¹‹ï¼ŒAgentäº†è§£è¿™ä¸ªä¸–ç•Œï¼Œèƒ½å¤Ÿæå‰é¢„çŸ¥å¹¶ä¸”ä¸éœ€è¦ä»»ä½•å°è¯•åœ°æŠŠæ‰€æœ‰æƒ…å†µæå‰è®¡ç®—æ¸…æ¥šå¾—å‡ºæœ€ä½³çš„policy

### Online Planning

- MDP
- Agent **may not** know **reward function or transition model**
- **==must try actions==** and states out to learn and get feedbacks
- Agent **use feedback to estimate an optimal policy** through a process known as reinforcement Learning
- ç®€è€Œè¨€ä¹‹ï¼ŒAgentæœ‰å¯èƒ½å¯¹è¿™ä¸ªä¸–ç•Œä¸€æ— æ‰€çŸ¥ã€‚éœ€è¦ä¸€æ­¥ä¸€æ­¥å°è¯•ï¼Œå¹¶ä¸”é€šè¿‡åœ¨å°è¯•ä¸­è·å¾—çš„åé¦ˆæ¥ä½œä¸ºä¸‹ä¸€æ¬¡çŠ¶æ€è½¬ç§»çš„è¾“å…¥ï¼Œä»è€Œè·å¾—æœ€ä½³policyã€‚

# Reinforcement Learning

## Definition

Reinforcement is a kind of online planning.

- Receive **feedback** in the form of **rewards** ä»¥ä¹‹å‰rewardä½œä¸ºåé¦ˆï¼Œå¾—åˆ°äº†å¤šå°‘å°±åé¦ˆç»™agent
- Agent's utility is defined by the reward function--- ä¹Ÿå°±æ˜¯è¯´ï¼Œagentä½¿ç”¨feedbackæ¥è¾“å‡ºutilityï¼Œæ¥estimate an optimal policy.
- Agent **must** act to **maximize expected rewards**.å¿…é¡»é€‰æ‹©èƒ½å¤Ÿmaximize rewardçš„ä¸¾åŠ¨ã€‚
- æ‰€æœ‰çš„è·å–åˆ°çš„ä¿¡æ¯ï¼Œéƒ½æ˜¯æ ¹æ® observed samples of outcomes!

<img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20220319233424976.png" alt="image-20220319233424976" style="zoom:50%;" />

### Model

- Still assume as a MDP
  - A set of states **s $\in$ S**
  - A set of **actions (per state) A**
  - A model **T(s,a,sâ€™)**
  - A reward function **R(s,a,sâ€™)**
- **Goal**: Still looking for policy $\pi(s)$
- New Problem: **ä¸çŸ¥T & R**
  - å¿…é¡»try out actions and states 

### Term

sample: (s,a,s',r) tuple

Episode: Collection of samples

Often, an agent continues to take actions and **collect samples** in succession until arriving at a terminal state. 

Agents typically **go through many episodes** during exploration in order to collect sufficient data needed for learning.

## Types

- **Model-based**: Attempts to **estimate the transition and reward functions** with samples(s,a,s') attained during exploration **before using these estimates to solve MDP**(normally with value or policy iteration) åˆ†ætransition å’Œ reward functions
  - Adaptive Dynamic Programming
- **Model-free**: Attempts to **estimate the values or Q-values of states** directly, without ever using any memory to construct a model of the rewards and transitions in MDP åˆ†æè®¡ç®—æ—¶å¾—åˆ°çš„valuesã€‚
  - Direct evaluation
  - Temporal difference learning
  - Q-learning
- **Passive**: **ç»™å®špolicy**ï¼Œè®©agentéµå¾ªè¿™ä¸ªpolicyå­¦ä¹ æ¯ä¸ªçŠ¶æ€è·å¾—çš„å€¼
  - Direct evaluation
  - Temporal  difference learning
  - Adaptive Dynamic Programming
- **Activate**ï¼šAgentèƒ½å¤Ÿä½¿ç”¨è·å¾—çš„feedbackæ¥**ä¸æ–­æ›´æ”¹policyç›´åˆ°è·å¾—optimal policy**
  - Action-Utility Learning
    - Q-learning: **learns a Q-function Q(s,a)**, agent can **choose** what to do in state s by finding the action with highest Q-value
  - Policy search
    - Agent learns a policy that maps directly from states to actions

## Model-based RL

### Direct evaluation

**Passive**

IDEAï¼šLean an **approximate model**(know/unknown),Solve for values as if the learned model were correct. é€‰æ‹©ä¸€ä¸ªç›¸è¿‘çš„æ¨¡å‹æ¥è¿›è¡Œå­¦ä¹ ï¼ŒæŠŠè¿™ä¸ªä½œä¸ºå‡†åˆ™æ¥è¿›è¡Œå€¼çš„æ±‚è§£

#### Steps

- Step1 : **Learn empirical MDP model**
  - è§‚å¯Ÿè¿™ä¸ªMDPäº§ç”Ÿçš„å¯¹åº”çš„actionçš„å¯¹åº”çš„outcome(s,a)
  - Normalizeï¼Œ**è·å¾—ä¼°ç®—çš„T(s,a,s')**
  - æ ¹æ®experiece(s,a,s') **è·å¾—å¯¹åº”çš„R(s,a,s')**
- Step2:  **Solve the learned MDP**
  - ä¾‹å¦‚ï¼Œä½¿ç”¨value iterationçš„æ–¹æ³•æ¥æ±‚è§£MDP

#### **Example**

- å¦‚ä¸‹ä¾‹å­ã€‚Policy $\pi$æ˜¯approximate modelã€‚ æ ¹æ®è¿™ä¸ªmodelæ¥æ”¶é›†ä¿¡æ¯ã€‚

- Episodesæ˜¯Agentè¿è¡Œåæ”¶é›†çš„samplesã€‚ä¸€å…±è¿è¡Œäº†å‡ æ¬¡ï¼Œæ”¶é›†äº†å‡ ç»„çš„Episodesã€‚

- Agent é€šè¿‡æ”¶é›†çš„æ•°æ®ï¼Œè¿›è¡ŒNormalizeï¼Œå¾—å‡ºTransition Function å’Œ Reward function.
  - Normalize æ–¹æ³•ï¼š\- dividing the count for **each observed tuple (s,a,s')** by the sum over the counts **for all instances where the agent was in Q-state (s,a)**. Count(s,a,s') / Count(s,a)
  - Normalization of counts **scales them such that they sum to one, allowing them to be interpreted as probabilities**

![image-20220320010055244](/Users/chengeping/Library/Application Support/typora-user-images/image-20220320010055244.png)

æ ¹æ®ä¸Šé¢çš„æ–¹æ³•è¿›è¡Œç»Ÿè®¡æœ‰ï¼š

| s    | a     | s'   | Count |
| ---- | ----- | ---- | ----- |
| A    | Exit  | x    | 1     |
| B    | East  | C    | 2     |
| C    | East  | A    | 1     |
| C    | East  | D    | 3     |
| D    | Exit  | x    | 3     |
| E    | North | C    | 2     |

æ ¹æ®MDPçš„å®šç†ï¼Œæœ‰ $T(s,a,s')=P(s'|a,s)ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®è®¡æ•°æ¥ä¼°ç®—transition functionï¼Œä»¥åŠæ ¹æ®è·å¾—çš„å€¼æ¥è·å¾—rewardã€‚äºæ˜¯ä¸Šè¿°æƒ…å†µæœ‰ï¼š

![image-20220320124740411](/Users/chengeping/Library/Application Support/typora-user-images/image-20220320124740411.png)

Summaryï¼š

By the law of large numbers, å½“æˆ‘ä»¬è·å–çš„æ ·æœ¬è¶Šæ¥è¶Šå¤šï¼Œtransition functionå’Œreward functionå°±ä¼š**è¶Šæ¥è¶Šè¶‹è¿‘çœŸå®æƒ…å†µ**ã€‚

**Whenever we see fit**, å½“æˆ‘ä»¬çœ‹åˆ°å’ŒçœŸå®æƒ…å†µç›¸ç¬¦åˆçš„æ—¶å€™ï¼Œå°±å¯ä»¥åœæ­¢agentçš„è®­ç»ƒï¼Œå¹¶ä¸”é€šè¿‡value/policy iteration**ç”Ÿæˆæ–°çš„policyã€‚**

#### Pros & Cons

ä¼˜ç‚¹ï¼šå¾ˆç®€å•ä¸”é«˜æ•ˆï¼Œåªéœ€è¦è®¡æ•°å’Œnormalizationå°±å¯ä»¥

ç¼ºç‚¹ï¼šä¿å­˜è®¡æ•°æˆæœ¬å¤ªé«˜ã€‚

### Adaptive Dynamic Programming(ADP)

è¿™æ˜¯**Passive** reinforcement learningæ–¹æ³•ã€‚

- è¿™ä¸ªæ–¹æ³•åˆ©ç”¨äº†Bellman equationçš„é™åˆ¶ã€‚
- Basically l**earns the transition model T and the reward function R** (model based)
- æ ¹æ®å­¦ä¹ åˆ°çš„T,R,æˆ‘ä»¬å¯ä»¥ä½¿ç”¨**policy evaluation**æ¥è¿›è¡Œè®¡ç®—ã€‚
- ==**Pollicy Evaluation**==ï¼š calculate utilities for some **fixed policies** until convergence ç»™å®špolicyï¼Œè®¡ç®—utilitiesï¼Œç›´åˆ°æ”¶æ•›ã€‚

#### Step

- Given policyï¼Œcalculate $U_i=U^{\pi i}$
- $U_i(s)=\sum _{s'}P(s'|s,\pi i(s))[R(s,\pi i(s),s')+\gamma U_i(s')]$
- R(s): å½“çœ‹åˆ°new sateï¼Œå°±å­˜å‚¨è·å¾—çš„rewardå€¼
- T(s,a,s') --- P(s'|s,$\pi (s)$): å…³æ³¨åˆ°è¾¾çš„æ¬¡æ•°ã€‚

![image-20220320201745029](/Users/chengeping/Library/Application Support/typora-user-images/image-20220320201745029.png)

#### Problem

- éœ€è¦è§£å†³å¤šä¸ªæ–¹ç¨‹ç»„
  - å¦‚æœçŠ¶æ€è¿‡å¤šï¼Œ**å¾ˆéš¾è®¡ç®—**
  - å¦‚æœåœ¨iterationæ—¶ä¸æ–­æ”¹è¿›policyï¼Œä¼šè½»æ¾äº›ï¼Ÿ

## Model-free RL

Agent å¯¹ç¯å¢ƒä¸€æ— æ‰€çŸ¥ï¼Œå¯¹transition modelä¸€æ— æ‰€çŸ¥ã€‚Agentæ ¹æ®æ¯ä¸ªçŠ¶æ€çš„ç›´æ¥è¡¨è±¡æ¥è¿›è¡Œå­¦ä¹ è‡ªå·±å¦‚ä½•behaveã€‚æ— éœ€é€šè¿‡modelå°±å¯ä»¥è·å¾—æœ€ä½³çš„policyã€‚

### Types

- Direct Utility Estimation
- Temporal difference learning
- Q-learning



## Passive Reinforcement Learning

ç»™å®šPolicyï¼Œè®©agentéµå¾ªpolicyæ¥take actionsï¼Œå¹¶ä¸”å­¦ä¹ æ¯ä¸ªçŠ¶æ€è·å¾—çš„å€¼

Simplified task: **Policy. Evaluation**

- Input: **a fixed policy $\pi(s)$**
- Goal: learn the **state values** ç›®çš„æ˜¯ä¸ºäº†è·å¾—æ¯ä¸ªçŠ¶æ€ä¸‹çš„value
- ä¸çŸ¥T(s,a,s'),ä¹Ÿä¸çŸ¥R(s,a,s')ï¼Œé€šè¿‡å¾ªç¯å­¦ä¹ æ¯ä¸ªçŠ¶æ€çš„å€¼ ä»è€Œå¾—åˆ°ã€‚

> æ³¨æ„ï¼šè™½ç„¶å’Œoffline learningå¾ˆåƒï¼Œä½†æ˜¯ä¸æ˜¯ï¼å®é™…ä¸Štake actionsäº†ï¼

### Types

- Direct utility estimation
- Adaptive dynamic programming(ADP)
- Temporal-difference(TD) learning

### Direct Utility Estimation(Direct Evaluation)

**Goal: Compute values for each state under $\pi$**ã€

#### Idea

Idea: Average observed sample values æ¯æ¬¡å–è§‚å¯Ÿåˆ°çš„æ ·æœ¬çš„å¹³å‡å€¼

- Agent acts according to policy $\pi$
- Every time the agent visits a state:
  - count the total rewards æ€»å…±çš„rewards
  - count the total number of times each state is visited æ€»å…±è¢«è®¿é—®çš„æ¬¡æ•°
- Average samples å–å¹³å‡å€¼

#### Example

Example1

![image-20220320162820334](/Users/chengeping/Library/Application Support/typora-user-images/image-20220320162820334.png)

æ ¹æ®ä¸Šå›¾ç»Ÿè®¡å¯å¾—

![image-20220320162838758](/Users/chengeping/Library/Application Support/typora-user-images/image-20220320162838758.png)

Example 2

![image-20220320163410967](/Users/chengeping/Library/Application Support/typora-user-images/image-20220320163410967.png)

æ ¹æ®ä¸Šå›¾ç»Ÿè®¡å¯å¾—ï¼š

| State | Total Rewards | Times Visited | U^pi^ (s) |
| ----- | ------------- | ------------- | --------- |
| A     | -10           | 1             | -10       |
| B     | 16            | 2             | 8         |
| C     | 27-11=16      | 4             | 4         |
| D     | 30            | 3             | 10        |
| E     | 8-12=-4       | 2             | -2        |

#### Pros and cons

**Pros**

- å®¹æ˜“ç†è§£
- ä¸éœ€è¦çŸ¥é“T,R(model free)
- å¤šæ¬¡sampleè®¡ç®—åï¼Œèƒ½å¤Ÿå¶çš„æ­£ç¡®å€¼ã€‚

**Cons**

- æ¯ä¸ªstateéƒ½éœ€è¦å•ç‹¬è®¡ç®—
- wastes information about state connections
- **éœ€è¦èŠ±è´¹å¾ˆé•¿æ—¶é—´ æ‰ä¼šconverge**

#### Why not use it

Bellman equation æœ‰ï¼šå¯è§ï¼Œæ˜¯è·Ÿneighborçš„çŠ¶æ€æ˜¯æœ‰å…³ç³»çš„ï¼Œè¿™äº›éƒ½éœ€è¦Tå’ŒRæ‰èƒ½å¾—åˆ°ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸çŸ¥ï¼Œå› æ­¤ï¼Œdirect evaluation æ— æ³•ç”¨ã€‚è¿™æ˜¯Bellman equation çš„é™åˆ¶ã€‚

![image-20220320192406769](/Users/chengeping/Library/Application Support/typora-user-images/image-20220320192406769.png)

### Temporal-Difference Learning

è¿™ä¸ªç®—æ³•æ—¢å¯ä»¥å…‹æœADPçš„é‡å¤è®¡ç®—é—®é¢˜ï¼Œåˆèƒ½æ›´åŠ å¿«é€Ÿåœ°æ”¶æ•›ã€‚

#### Idea

Still fixed policy, still doing evaluation

Learn from **every experiece**

**On-policy learning**: learn the values for a specific policy before deciding whether that policy is suboptimal and needs to be updated.

#### Steps

Update V(s) each time we experience a transition (s, a, sâ€™, r)  ä¸€æœ‰å˜åŒ–å°±update

- Move values toward value of whatever successor occurs: **running average** æ›´æ­£å€¼

  - **$\alpha$æ˜¯learning rate**å–å€¼åœ¨0ï¼Œ1ä¹‹é—´ã€‚å¯ä»¥ç†è§£ä¸ºã€‚åœ¨åŸåŸºç¡€ä¸Šï¼Œæ¯æ¬¡ä¿®æ­£error*learning rateï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡æ›´æ­£çš„æ˜¯errorï¼

  <img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20220321000400580.png" alt="image-20220321000400580" style="zoom:50%;" />

- Uses an exponential moving average with sampled values until convergence to the true values of states under ğœ‹ 

  - Makes **recent samples more important**

    å…¶å®å¯¹äºæ¯ä¸€ä¸ªéƒ½æœ‰ï¼š<img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20220321010530242.png" alt="image-20220321010530242" style="zoom:33%;" />

    äºæ˜¯æœ‰

    <img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20220321010551602.png" alt="image-20220321010551602" style="zoom:33%;" />

    å¯ä»¥çœ‹å‡ºï¼Œæœ€è¿‘çš„ä¸€æ¬¡å€¼æ¬¡å•Šæ˜¯æœ€é‡è¦çš„ï¼Œå½±å“ä¸‹ä¸€æ¬¡çš„ã€‚

  - Turn Î± **into a function** that decreases as the # of times a state **has been visited increases**æŠŠaå˜æˆå¯å˜çš„ï¼Œè€Œä¸æ˜¯ä¸€æˆä¸å˜çš„ã€‚

  - Decreasing learning rate (Î±) can give converging averages ä¸€ç‚¹ä¸€ç‚¹é™ä½å­¦ä¹ é€Ÿåº¦

  - > Itâ€™s **typical to start out with learning rate of Î± = 1**, accordingly assigning *V* Ï€ (*s*) to whatever the first sample happens to be, **and slowly shrinking it towards 0**, at which point all subsequent samples **will be zeroed out and stop affecting our model of *V* Ï€ (*s*).**

#### Code

![image-20220321005905445](/Users/chengeping/Library/Application Support/typora-user-images/image-20220321005905445.png)

#### Example

![image-20220321010345722](/Users/chengeping/Library/Application Support/typora-user-images/image-20220321010345722.png)



#### Pros & Cons

**Pros**

- ä¸éœ€è¦Transition model å°±å¯ä»¥è¾¾åˆ°updateçš„æ•ˆæœ
- æ¯”ADPæ”¶æ•›æ…¢ä½†æ˜¯**much higher variability**
- updateæ–¹å¼ç®€å•ï¼Œå¹¶ä¸”è®¡ç®—ä¹Ÿæ¯”ADPå°‘ã€‚

- learn at every step, hence using information about state transitions as we get them since weâ€™re using **iteratively updated versions** of *V* Ï€ (*s*â€² ) in our samples rather than waiting until the end to perform any computation.
- give exponentially **less weight to older**, potentially less accurate samples.
- converge to learning true state values **much faster with fewer episodes than direct evaluation.**



## Active Reinforcement Learning

Agent **decides what actions to take and learns the underline MDP**

The agent attempts to find an optimal policy by **exploring** different actions in the world

Agentèƒ½å¤Ÿä½¿ç”¨è·å¾—çš„feedbackæ¥**ä¸æ–­æ›´æ”¹policyç›´åˆ°è·å¾—optimal policy**

> ä¹‹å‰çš„TDè™½ç„¶å·²ç»åšåˆ°çš„äº†å¾ˆå¥½çš„è¶‹è¿‘çš„æ•ˆæœï¼Œä½†æ˜¯å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šåªèƒ½æ¨¡ä»¿å·²ç»å­˜åœ¨çš„policyã€‚ä½†æ˜¯å½“æˆ‘ä»¬éœ€è¦å¯»æ‰¾æ–°çš„policyçš„æ—¶å€™ï¼Œå°±æ— æ³•åšåˆ°updateã€‚

äºæ˜¯æœ‰äº†Active Reinforcement Learning

### Idea

<img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20220321020652853.png" alt="image-20220321020652853" style="zoom: 25%;" /><img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20220321020703414.png" alt="image-20220321020703414" style="zoom: 25%;" />

Don't know T & R, choose actions NOW!

**Goal**: learn the optimal policy/values

**Steps**: **Learn Q-Valueï¼Œ è€Œä¸æ˜¯valueï¼**Q-valueèƒ½å¤Ÿçœ‹å‡ºé€‰æ‹©çš„æ–¹å‘ã€‚Makes action selection model-free tooï¼

åŒºåˆ«ï¼šQ valueå¯»æ‰¾çš„æ˜¯å„ä¸ªæ–¹å‘ä¸Šè¾ƒå¤§çš„å€¼ã€‚

![image-20220321153451317](/Users/chengeping/Library/Application Support/typora-user-images/image-20220321153451317.png)



### Types

- Action-utility learning
  - Q-learning:Agent learns Q-function Q(s,a), agentå¯ä»¥é€‰æ‹©highest Q-valueçš„action
- Policy search
  - Agent learns a policy ğœ‹ that maps directly from states to actions

### Terms

#### On-policy VS off-policy

- On-policy: 
  - Learns the values for a specific policy **before** deciding **whether that policy is suboptimal and needs to be updated**
  - TD / direct utility estimation

- Off-policy:
  - Learns an optimal policy **even when taking suboptimal actions**
  - Q-learning / approximate Q-learning

#### Exploration VS Exploitation

An agent must make a trade off between **exploitation** of the current best action to maximize its short-term reward and **exploration** of previously **unknown states** to gain information that can lead to a change in policy (greater rewards) Agentå¿…é¡»ä½œå‡ºæƒè¡¡ï¼šåˆ©ç”¨å½“å‰çš„æœ€å¥½çš„actionæ¥è·å¾—çŸ­æœŸæœ€å¥½æ•ˆç›Š æˆ–è€… æ¢ç´¢æ–°çš„æœªçŸ¥stateä»è€Œè·å¾—æ›´å¥½çš„æ•ˆç›Š

- exploitationï¼šalways chooses the best reward action(é€‰æ‹©å½“å‰æœ€å¥½)
- Exploration:  always select random moves(é€‰æ‹©æœªçŸ¥)

å‡ ç§æ–¹æ³•

1. Naive approachï¼šéšæ„é€‰æ‹©
2. Greedy approachï¼š è´ªå¿ƒï¼Œä½†overestimate rewards forunexplored states (è´ªå¿ƒçš„æ˜¯æœªè®¿é—®è¿‡çš„åœ°æ–¹ï¼Œæƒå€¼æ›´é«˜)

Greedy in the limit of infinite exploration(**GLIE**) æœ‰é™æ¢ç´¢ä¸­è¿›è¡Œè´ªå¿ƒ

### GLIE Schemes

**IDEA**

- simplest: Agent chooses a **random action** at time step t with **probability 1/t** and to follow the greedy policy otherwise.
  - Every time step, flip a coin
  - With (small) probability e, act randomly
  - With (large) probability 1-e, act on current policy

ä½†æ˜¯ä½¿ç”¨è¿™ä¸ªç®€å•çš„è´ªå¿ƒæ–¹æ³•ï¼Œæœ‰ä¸ªé—®é¢˜ï¼šè™½ç„¶è¾¾åˆ°äº†è®¿é—®å…¶ä»–åœ°æ–¹çš„ç›®çš„ï¼Œä½†æ˜¯å¯èƒ½ä¼šä¸€åªåœ¨ä¸€ä¸ªåœ°æ–¹æ‰“è½¬ã€‚è§£å†³æ–¹æ¡ˆï¼šæ¯æ¬¡é™ä½probability e/ä½¿ç”¨exploration function

- Better: give some weight to actions that the agent **has not tried very often**, while **tending to avoid actions that are believed to be of low utility** --- ç»™æœªè®¿é—®è¿‡çš„åœ°æ–¹èµ‹äºˆé«˜æƒå€¼ï¼Œå¯¹è‚¯å®šä¼šæœ‰low utilityçš„åœ°æ–¹é¿å…æ‰§è¡Œæ­¤åŠ¨ä½œã€‚
  - ![image-20220321164540856](/Users/chengeping/Library/Application Support/typora-user-images/image-20220321164540856.png)
  - ==Mix Exploration & Expoitation==

![image-20220321150631694](/Users/chengeping/Library/Application Support/typora-user-images/image-20220321150631694.png)

### Safe Exploration 

é‚£ä¹ˆå¦‚ä½•ä¿è¯å®‰å…¨çš„æ¢ç´¢å‘¢ï¼Ÿ

- Simplestï¼šNegative rewards æ˜¯å¯ä»¥èµ‹å€¼negativeçš„
  - Many actions are irreversible
    - Absorbing state: no actions have any effect and no rewards are received
    - Self-driving car: large negative rewards (Car crashes) å¯¹äºè‡ªåŠ¨é©¾é©¶ï¼Œå¯ä»¥èµ‹äºˆéå¸¸å¤§çš„negativeå€¼
    - Learn how the traffic lights work and explore it by ignoring red light é€šè¿‡é—¯çº¢ç¯æ¥è·å–å€¼
- **Better idea**ï¼šchoose a policy that works reasonably well **for the whole range of models** and thus have a reasonable chance of being the true model é€‰å¥½çš„ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ

### Q-learning

Q-learning: sample-based Q-value iteration

> Q-value çš„è®¡ç®— ä¸€èˆ¬éœ€è¦transition modelå’Œreward functionï¼Œä½†æ˜¯Q-learningå…‹æœäº†è¿™ä¸€ç‚¹ï¼Œä½¿ç”¨è¿­ä»£çš„æ–¹å¼ï¼Œç›´æ¥è·å¾—å½“ä¸‹statesçš„å€¼ã€‚Q-learning is model-free

#### Steps

Learn Q-values as you go:

![image-20220321161412026](/Users/chengeping/Library/Application Support/typora-user-images/image-20220321161412026.png)

- Receive a sample (s,a,sâ€™,r) 

- Consider your old estimate: Q(s,a)

- Consider your new sample estimate: $sample=R(s,a,s')+\gamma ^{max}_{a'}Q(s',a')$ æ¯æ¬¡è¿›è¡Œä¼°ç®—ï¼Œé€‰æ‹©valueæœ€å¤§çš„å€¼ã€‚

- **Incorporate the new estimate into a running average:**

  $Q(s,a)=Q(s,a)+\alpha[sample-Q(s,a)]$ åŒç†ä¸º

  $Q(s,a)=(1-\alpha)Q(s,\alpha)+(\alpha)sample$ 

  æœ€åæ›´æ–°Q-value

#### Pros & Cons

**Pros**

- Q-learning **converges to optimal policy** -- even if youâ€™re acting suboptimally! -**Off-policy learning**
- **It doesnâ€™t matter how the agent selects actions**
- Off-policy!: å’Œon-policyä¸åŒï¼Œå³ä½¿ç›®å‰è¿è¡Œçš„policyä¸æ˜¯æœ€ä½³çš„ï¼Œä¹Ÿèƒ½è·å¾—æœ€å¥½çš„

**Cons**

- The agent must explore enough
- The agent have to eventually make the learning rate small enough but not decrease it too quickly
- ç®€è€Œè¨€ä¹‹ï¼šéœ€è¦éå†æ¬¡æ•°è¿‡å¤šã€‚å¹¶ä¸”æœ€ålearning rateè¿˜è¦è¶³å¤Ÿå°ï¼Œä½†æ˜¯æ¯æ¬¡å‡å°çš„é€Ÿåº¦åˆä¸èƒ½å¤ªå¿«ï¼Œä¸å¥½æ§åˆ¶ï¼

#### code

![image-20220321162043001](/Users/chengeping/Library/Application Support/typora-user-images/image-20220321162043001.png)

#### Probelm

Even if you learn the optimal policy, you still make mistakes along the way!

- Regret is a measure of your total mistake cost: the difference between your (expected) rewards, including youthful suboptimality, and optimal (expected) rewards
- Minimizing regret goes beyond learning to be optimal â€“ it requires optimally learning to be optimal 
- Example: random exploration and exploration functions both end up optimal, but random exploration has higher regret



### Generalization

Q-learning ä¼šä¿å­˜ä¸€å¼ å®Œæ•´çš„q-valueè¡¨æ ¼ï¼Œéœ€è¦å¤§é‡çš„q-valueï¼Œstatesï¼Œè¿™å¯¹memoryè´Ÿæ‹…å¾ˆå¤§ã€‚è€Œä¸”åœ¨çœŸå®æƒ…å†µä¸‹ï¼Œ æˆ‘ä»¬ä¹Ÿä¸å¯èƒ½å­¦ä¹ æ‰€æœ‰çš„state

> This means we canâ€™t visit all states during training and canâ€™t store all Q-values even if we could for lack of memory.

å› æ­¤ï¼Œéœ€è¦generalizationï¼

æˆ‘ä»¬å¯ä»¥æ ¹æ®ç»éªŒï¼Œå­¦ä¹ ä¸€éƒ¨åˆ†çš„training states

**Solution**ï¼šdescribe a state using a vector of features (properties)

The key to generalizing learning experiences is the **feature-based representation** of states, which represents each state as a vector known as a feature vector.

- using a weighted linear combination of features f1,...,fn:
- å¦‚ä¸‹ï¼Œ[f1(s),f2{s}â€¦â€¦fn(s)]æ˜¯feature vectorï¼Œ[f1(s,a), f2(s,a) â€¦â€¦fn(s,a)]æ˜¯Q-state vectorsã€‚[w1,w2,w3â€¦â€¦wn]æ˜¯æƒé‡vector
- ![image-20220321174403372](/Users/chengeping/Library/Application Support/typora-user-images/image-20220321174403372.png)

**Advantage**ï¼šour experience is summed up in a few powerful numbers

**Disadvantage**ï¼šstates may share features but actually be very different in value!



### Approximating Q-Learning

- ç›¸æ¯”Q-learningï¼Œ æ›´åŠ generalizedï¼Œå¹¶ä¸”more memory-efficient
- åªéœ€è¦å­˜å‚¨ weight vectorå³å¯ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—ç›´æ¥å¾—å‡ºå¯¹åº”çš„Qvalue
- å®ƒå¯¹äºä¸€äº›generalçš„situationsè¿›è¡Œå­¦ä¹ ï¼Œå¹¶ä¸”å¯ä»¥å¼•ç”¨åˆ°å…¶ä»–ç›¸ä¼¼æƒ…å½¢ä¸Š
- æ¯ä¸€ä¸ªçŠ¶æ€éƒ½å¯ä»¥ç”¨feature vectoræ¥è¡¨ç¤ºã€‚

æ ¹æ®linear Q-learning æœ‰ï¼š<img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20220321215946666.png" alt="image-20220321215946666" style="zoom: 25%;" />

defining difference as: ï¼ˆè§‚å¯Ÿåˆ°çš„sample-ä¹‹å‰Q-valueï¼‰

<img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20220321214955377.png" alt="image-20220321214955377" style="zoom:50%;" />

Exact Q: $Q(s,a)=Q(s,a)+\alpha [difference]$

Approximate Q: <img src="/Users/chengeping/Library/Application Support/typora-user-images/image-20220321215152320.png" alt="image-20220321215152320" style="zoom:50%;" />

ç”±æ­¤å¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦å­˜å‚¨qvalueï¼Œå¯ä»¥é€šè¿‡è®¡ç®—å¾—å‡ºï¼Œåªéœ€è¦å­˜å‚¨weightå³å¯ã€‚

#### Steps

- Comopute Q-value é¦–å…ˆè®¡ç®—å„ä¸ªactionçš„Q-valueï¼Œä¸ºäº†å¯»æ‰¾æœ€é«˜çš„Valueä»è€Œä½œå‡ºaction
- Make action based on the highest Q-value æ ¹æ®å½“å‰ä½œå‡ºaction
- Compute Q-values and sample from the reward æ ¹æ®å¾—åˆ°çš„rewardï¼Œè®¡ç®—sample
- Compute difference è®¡ç®—difference
- Update weight vector æ›´æ–°

#### Example

![image-20220321223622388](/Users/chengeping/Library/Application Support/typora-user-images/image-20220321223622388.png)

### Policy search

Problemï¼šæœ‰æ—¶å€™ä¼šå‘ç°ï¼Œè¿è¡Œæœ€å¥½çš„å¹¶ä¸æ˜¯Q valueæœ€å¥½çš„policy

è§£å†³æ–¹æ¡ˆï¼š learn policies that maximize rewards, not the values that predict them

#### Idea

**fine-tune the policy** as long as its performance improves, then stop

Policy search: start with an ok solution (e.g. Q-learning) then fine-tune by hill climbing on feature weights

- Q-learning: find the value that is close to Q*
- Policy search: find the value that results in good performance

Start adjust the parameters to improve the policy by hill climbing on feature weights

## Comparison

### ADP VS TD

ADP

- æ”¶æ•›æ›´å¿«
- å’ŒçœŸå®å€¼ç›¸å·®ä¸ä¼šå¾ˆå¤šï¼Œæ›´å‡†ç¡®ã€‚
- ä¼šæ ¹æ®æ‰€æœ‰successorsï¼Œæ ¹æ®probabilitesæ¥è°ƒæ•´è‡ªå·±çš„æƒé‡
- Makes as many as it needs to restore consistency between the utility estimates *U* and the transition model *T*

TD

- ä¸éœ€è¦æ„å»ºtransitionmodel
- æ ¹æ®è§‚å¯Ÿåˆ°çš„successoræ¥è°ƒæ•´è‡ªå·±çš„state
- ä¸€æ¬¡å€¼æ›´æ”¹ä¸€ä¸ªstate
- updateæ›´ç®€å•ï¼Œè®¡ç®—æ›´ç®€å•ã€‚



### Model free VS Model Based

Model based

- éœ€è¦æ›´æ–°Tå’ŒRï¼Œæ¯æ¬¡éƒ½è¦è¿›è¡Œè®¡ç®—
- ä½¿ç”¨è¿™äº›è·å¾—çš„ä¼°ç®—æ¥è§£å†³MDPé—®é¢˜ã€‚

Model free

- ä¸éœ€è¦Tï¼ŒRï¼Œç›´æ¥ä¼°ç®—value/q-value

### Direct utility estimation/ADP/TD

|                | Direct utility estimation     | ADP                                                 | TD                                                           |
| -------------- | ----------------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| Types          | Model free                    | model based                                         | Model free                                                   |
| Implementation | Easy                          | Harder                                              | Easy                                                         |
| Converge       | very slow/ long time to learn | Fast                                                | Medium                                                       |
| Pros           |                               | **Fully** exploits Bellman constraints              | **Partially** exploits Bellman constraints (adjustsstatetoâ€œagreeâ€with observed successor (Not all possible successors)) |
| Cons           |                               | Each update is a full policy evaluation (Expensive) |                                                              |


# Unsupervised Learning

## Clustering

### **Good clustering**

- 高 集群内的相似度
- 低 集群间的相似度

### 应用：

- 作为了解数据分布的工具
- 其他算法的前置算法

## K-means

> mother of all clustering methods

给定N个点，将他们分入k类

### 过程

中心：每个类都是根据点的mean聚集起来的

#### 算法过程：

1. 随意生成点的中心-k个 （k是**人为指定的**，没有特定答案，通常通过设定多个k值然后计算多个ratio来）
2. 重复直到这个中心不再变化
   1. 将每个点归类到最近的中心
   2. 计算每个归类到这个点的所有点的新的mean，作为新的中心。

#### **优点**：

原理简单，容易实现，聚类效果好

#### **缺点**：

对噪声敏感

### 使用

```python
import sklearn
import numpy as np
from sklearn.cluster import KMeans

seed_data_pca = pd.read_csv('seeds_dataset.txt', delim_whitespace=True, header=None)

X = seed_data_pca.values[:,:-1]
y = seed_data_pca.values[:,7]

#fit kmeans
kmeans_cluster_algo = KMeans(n_clusters=3)
kmeans_cluster_algo.fit(X)

print(kmeans_cluster_algo.labels_[::10]) #每十个数据，打印它所在的聚类。
out： [1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]

import collections
print "Clusters (result of k-means)"    #打印的是聚类分析所聚的类
print collections.Counter(kmeans_cluster_algo.labels_) #Print the frequency of elements in a numpy array
print "Ground truth"
print collections.Counter(iris.target) # ditto

out：
Clusters (result of k-means)
Counter({1: 77, 0: 72, 2: 61})
Ground truth
Counter({0: 70, 1: 70, 2: 70})
```



#### 改进

**K-medoids**

算法和k-means类似，但是选择的中心点不是通过计算mean，而是寻找中心的点。

**Probabilistic clustering**

一个点可以属于多个点的聚类方式

## Hierarchical Clustering

优点

Produces a tree of clusters

Does not require number of clusters as input

缺点

Also heuristic

Does not scale well




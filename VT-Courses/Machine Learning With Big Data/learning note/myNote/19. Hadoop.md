# Hadoop

https://www.cnblogs.com/qingyunzong/p/8487180.html

具体细节见这里（或者以后有时间我自己自学慢慢扩充）以下重点记录mapreduce

当所读取的文件超过内存大小时，hadoop的出现就显得很重要。hadoop可以横向扩展。

## Map Reduce

总体流程是：

- 顺序读取大量数据
- Map：抓取我们要的内容
- Group by key： sort - > shuffle
- Reduce: 合并
- Write the result

# Spark